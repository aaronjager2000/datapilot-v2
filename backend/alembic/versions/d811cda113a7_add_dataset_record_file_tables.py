"""add_dataset_record_file_tables

Revision ID: d811cda113a7
Revises: 11e02838163f
Create Date: 2025-12-10 00:31:51.070303

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = 'd811cda113a7'
down_revision = '11e02838163f'
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('datasets',
    sa.Column('name', sa.String(length=255), nullable=False, comment='Dataset name (user-provided or derived from filename)'),
    sa.Column('description', sa.Text(), nullable=True, comment='Optional description of the dataset'),
    sa.Column('file_name', sa.String(length=255), nullable=False, comment='Original filename of uploaded file'),
    sa.Column('file_size', sa.BigInteger(), nullable=False, comment='File size in bytes'),
    sa.Column('file_hash', sa.String(length=64), nullable=False, comment='SHA-256 hash for deduplication and integrity checking'),
    sa.Column('file_path', sa.String(length=512), nullable=False, comment='S3 key or file path where the dataset is stored'),
    sa.Column('status', sa.Enum('UPLOADING', 'PROCESSING', 'READY', 'FAILED', name='dataset_status_enum'), nullable=False, comment='Current processing status of the dataset'),
    sa.Column('processing_error', sa.Text(), nullable=True, comment='Error message if processing failed'),
    sa.Column('row_count', sa.Integer(), nullable=True, comment='Number of rows in the dataset'),
    sa.Column('column_count', sa.Integer(), nullable=True, comment='Number of columns in the dataset'),
    sa.Column('schema_info', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Schema metadata including column names, types, and statistics'),
    sa.Column('created_by', sa.UUID(), nullable=True, comment='User who created/uploaded this dataset'),
    sa.Column('organization_id', sa.UUID(), nullable=False, comment='The organization this record belongs to'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier for this record'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was created'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when this record was last updated'),
    sa.ForeignKeyConstraint(['created_by'], ['users.id'], ondelete='SET NULL'),
    sa.ForeignKeyConstraint(['organization_id'], ['organizations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id'),
    comment='Dataset table for multi-tenant data'
    )
    op.create_index(op.f('ix_datasets_created_by'), 'datasets', ['created_by'], unique=False)
    op.create_index(op.f('ix_datasets_file_hash'), 'datasets', ['file_hash'], unique=False)
    op.create_index(op.f('ix_datasets_name'), 'datasets', ['name'], unique=False)
    op.create_index(op.f('ix_datasets_organization_id'), 'datasets', ['organization_id'], unique=False)
    op.create_index(op.f('ix_datasets_status'), 'datasets', ['status'], unique=False)
    op.create_table('files',
    sa.Column('uploaded_by', sa.UUID(), nullable=True, comment='User who uploaded this file'),
    sa.Column('file_name', sa.String(length=255), nullable=False, comment='Original filename'),
    sa.Column('file_size', sa.BigInteger(), nullable=False, comment='File size in bytes'),
    sa.Column('file_hash', sa.String(length=64), nullable=False, comment='SHA-256 hash for deduplication and integrity'),
    sa.Column('file_path', sa.String(length=512), nullable=False, comment='Storage path or S3 key'),
    sa.Column('mime_type', sa.String(length=127), nullable=False, comment='MIME type of the file'),
    sa.Column('storage_location', sa.Enum('LOCAL', 'S3', 'R2', name='storage_location_enum'), nullable=False, comment='Where the file is stored'),
    sa.Column('dataset_id', sa.UUID(), nullable=True, comment='Dataset created from this file (set after processing)'),
    sa.Column('organization_id', sa.UUID(), nullable=False, comment='The organization this record belongs to'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier for this record'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was created'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when this record was last updated'),
    sa.ForeignKeyConstraint(['dataset_id'], ['datasets.id'], ondelete='SET NULL'),
    sa.ForeignKeyConstraint(['organization_id'], ['organizations.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['uploaded_by'], ['users.id'], ondelete='SET NULL'),
    sa.PrimaryKeyConstraint('id'),
    comment='File table for multi-tenant data'
    )
    op.create_index(op.f('ix_files_dataset_id'), 'files', ['dataset_id'], unique=False)
    op.create_index(op.f('ix_files_file_hash'), 'files', ['file_hash'], unique=False)
    op.create_index(op.f('ix_files_organization_id'), 'files', ['organization_id'], unique=False)
    op.create_index(op.f('ix_files_storage_location'), 'files', ['storage_location'], unique=False)
    op.create_index(op.f('ix_files_uploaded_by'), 'files', ['uploaded_by'], unique=False)
    op.create_table('records',
    sa.Column('dataset_id', sa.UUID(), nullable=False, comment='Dataset this record belongs to'),
    sa.Column('row_number', sa.Integer(), nullable=False, comment='Original row position in the uploaded file (1-indexed)'),
    sa.Column('data', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='Key-value pairs for each column in the row'),
    sa.Column('is_valid', sa.Boolean(), nullable=False, comment='Whether this record passed validation'),
    sa.Column('validation_errors', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Array of validation error messages'),
    sa.Column('organization_id', sa.UUID(), nullable=False, comment='The organization this record belongs to'),
    sa.Column('id', sa.UUID(), nullable=False, comment='Unique identifier for this record'),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when the record was created'),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False, comment='Timestamp when this record was last updated'),
    sa.ForeignKeyConstraint(['dataset_id'], ['datasets.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['organization_id'], ['organizations.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id'),
    comment='Record table for multi-tenant data'
    )
    op.create_index(op.f('ix_records_dataset_id'), 'records', ['dataset_id'], unique=False)
    op.create_index('ix_records_dataset_org', 'records', ['dataset_id', 'organization_id'], unique=False)
    op.create_index('ix_records_dataset_row', 'records', ['dataset_id', 'row_number'], unique=False)
    op.create_index(op.f('ix_records_is_valid'), 'records', ['is_valid'], unique=False)
    op.create_index(op.f('ix_records_organization_id'), 'records', ['organization_id'], unique=False)
    op.create_index('ix_records_valid', 'records', ['dataset_id', 'is_valid'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index('ix_records_valid', table_name='records')
    op.drop_index(op.f('ix_records_organization_id'), table_name='records')
    op.drop_index(op.f('ix_records_is_valid'), table_name='records')
    op.drop_index('ix_records_dataset_row', table_name='records')
    op.drop_index('ix_records_dataset_org', table_name='records')
    op.drop_index(op.f('ix_records_dataset_id'), table_name='records')
    op.drop_table('records')
    op.drop_index(op.f('ix_files_uploaded_by'), table_name='files')
    op.drop_index(op.f('ix_files_storage_location'), table_name='files')
    op.drop_index(op.f('ix_files_organization_id'), table_name='files')
    op.drop_index(op.f('ix_files_file_hash'), table_name='files')
    op.drop_index(op.f('ix_files_dataset_id'), table_name='files')
    op.drop_table('files')
    op.drop_index(op.f('ix_datasets_status'), table_name='datasets')
    op.drop_index(op.f('ix_datasets_organization_id'), table_name='datasets')
    op.drop_index(op.f('ix_datasets_name'), table_name='datasets')
    op.drop_index(op.f('ix_datasets_file_hash'), table_name='datasets')
    op.drop_index(op.f('ix_datasets_created_by'), table_name='datasets')
    op.drop_table('datasets')
    # ### end Alembic commands ###
